#!/usr/bin/env python3
"""Verify Quote Baseline performance on novel-dialogism with quote-based labels."""

import sys
sys.path.insert(0, '/home/bor/Projects/tei-xml/annotation-ml/src')

import json
from pathlib import Path
from speech_detection.data.quote_based_extractor import QuoteBasedSpeechExtractor
from speech_detection.data.splits import load_splits
from speech_detection.models import QuoteBaselineModel, QuoteBaselineConfig
from speech_detection.evaluation import compute_f1, compute_precision, compute_recall

print("="*70)
print("VERIFYING QUOTE BASELINE ON NOVEL-DIALOGISM")
print("="*70)

# Load splits
splits_path = '../datasets/novel-dialogism/splits_quote_based.json'
splits = load_splits(splits_path)

# Initialize quote-based parser
parser = QuoteBasedSpeechExtractor()

# Load test data
corpus_name = 'novel-dialogism'
corpus_dir = Path('../datasets/novel-dialogism')
split_name = 'test'

file_names = splits['corpora'][corpus_name][split_name]
print(f"\nLoading {split_name} split: {len(file_names)} files")

data = []
for file_name in file_names:
    file_path = corpus_dir / split_name / file_name
    if not file_path.exists():
        continue

    try:
        paragraphs = parser.parse_and_label(str(file_path))
        data.extend(paragraphs)
    except Exception as e:
        print(f"  Error parsing {file_name}: {e}")
        continue

print(f"Loaded {len(data)} paragraphs")

# Run baseline
print("\nRunning Quote Baseline predictions...")
baseline = QuoteBaselineModel(QuoteBaselineConfig())
predictions = baseline.predict_paragraphs(data)

# Compute metrics
true_labels = [p['bio_labels'] for p in data]
pred_labels = [p.predicted_bio_labels for p in predictions]

# Calculate overlap
gold_speech = sum(sum(1 for l in labels if l != 'O') for labels in true_labels)
pred_speech = sum(sum(1 for l in labels if l != 'O') for labels in pred_labels)
overlap = sum(
    sum(1 for g, p in zip(t, p) if g != 'O' and p != 'O')
    for t, p in zip(true_labels, pred_labels)
)

f1 = compute_f1(true_labels, pred_labels)
precision = compute_precision(true_labels, pred_labels)
recall = compute_recall(true_labels, pred_labels)

print("\n" + "="*70)
print("BASELINE RESULTS")
print("="*70)
print(f"Gold speech tokens:  {gold_speech}")
print(f"Pred speech tokens:  {pred_speech}")
print(f"Overlap:             {overlap}")
print(f"\nF1:        {f1:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")

print("\nExpected: Recall = 1.0000 (labels generated by same baseline)")
print(f"Actual:   Recall = {recall:.4f}")

if recall >= 0.999:
    print("\n✅ VERIFICATION PASSED: Quote Baseline achieves perfect recall")
else:
    print("\n❌ VERIFICATION FAILED: Quote Baseline should have perfect recall")
    print("   This suggests labels are not quote-based or there's a bug")
