# Model Performance Comparison - Production Configs (2026-02-05)

## Executive Summary

Comparing three approaches for speech detection in TEI XML documents:
1. **Quote Baseline** (Rule-based)
2. **CRF Fast** (Statistical ML)
3. **DistilBERT Baseline** (Transformer)

**Key Finding**: CRF model achieves F1=0.365 with 95% CI [0.198, 0.519], significantly outperforming the baseline.

---

## 1. Quote Baseline (Rule-Based)

### Configuration
```yaml
Model Type: Rule-based quote matching
Features:
  - Detects various quote characters (straight, smart quotes, guillemets)
  - Handles multi-paragraph quotes
  - Handles nested quotes
  - Apostrophe detection (Cameron's, don't, etc.)
Speech Label: DIRECT
```

### Evaluation
```
Dataset: Test split from splits.json
Corpora: 7 corpora total
  - wright-american-fiction
  - victorian-women-writers
  - indiana-magazine-history
  - indiana-authors-books
  - brevier-legislative
  - tei-texts
  - novel-dialogism

Total Documents: 1,629
Total Paragraphs: 444,790
```

### Results
| Metric | Value |
|--------|-------|
| F1 | 0.0003 |
| Precision | 0.0002 |
| Recall | 0.0075 |

### Analysis
- **Massive over-prediction**: Marks ANY quoted text as speech
- Cannot distinguish dialogue from:
  - Book titles
  - Emphasized words
  - Citations
  - Internal monologues
- **Fundamental limitation**: Rule-based approach cannot learn context

---

## 2. CRF Model (Fast Config)

### Configuration
```yaml
Model: Conditional Random Fields
Implementation: sklearn-crfsuite

Feature Engineering:
  - Character n-grams: [2, 5] (captures patterns like "said, "qu, etc.)
  - Context window: 2 tokens (look at previous/next words)
  - Speech verb features: 100+ common speech verbs (said, replied, asked, etc.)
  - Orthographic features: Capitalization, word shapes

Hyperparameters:
  c1 (L1 regularization): 0.5
  c2 (L2 regularization): 0.5
  max_iterations: 100

Cross-Validation:
  n_folds: 5 (document-level to prevent data leakage)
  n_bootstrap: 2000 (for 95% confidence intervals)
  n_jobs: 2 (parallel fold evaluation)
  random_state: 42
```

### Training Data
```
Dataset: Train split from splits.json
Total Documents: 7,570 (from 7 corpora)
Total Paragraphs: 22,749

Corpus Breakdown:
  - wright-american-fiction: 2,013 docs
  - victorian-women-writers: 139 docs
  - indiana-magazine-history: 5,102 docs
  - indiana-authors-books: 275 docs
  - brevier-legislative: 13 docs
  - tei-texts: 9 docs
  - novel-dialogism: 19 docs

Training Time: ~20 minutes (5-fold CV with 2000 bootstrap)
Device: CPU (multi-core)
```

### Results
| Metric | Estimate | 95% CI | SE | Fold Range |
|--------|----------|--------|-----|------------|
| **F1** | **0.365** | **[0.198, 0.519]** | 0.096 | 0.051 - 0.636 |
| **Precision** | **0.686** | **[0.302, 0.991]** | 0.197 | 0.027 - 1.000 |
| **Recall** | **0.336** | **[0.251, 0.422]** | 0.051 | 0.225 - 0.470 |

### Per-Fold Breakdown
```
Fold 1: F1=0.296, Precision=0.430, Recall=0.225
Fold 2: F1=0.424, Precision=0.985, Recall=0.270
Fold 3: F1=0.636, Precision=0.986, Recall=0.470
Fold 4: F1=0.416, Precision=1.000, Recall=0.263
Fold 5: F1=0.051, Precision=0.027, Recall=0.449
```

### Analysis
- **High precision** (0.686): When it predicts speech, it's usually correct
- **Moderate recall** (0.336): Finds about 1/3 of actual speech tokens
- **Large variance across folds**: Fold 3 (F1=0.636) vs Fold 5 (F1=0.051)
  - Suggests data heterogeneity across corpora
  - Some corpora may be harder than others
- **Confidence interval is wide**: [0.198, 0.519] due to:
  - High variance across folds
  - Class imbalance (speech tokens are only ~0.7% of data)

### Comparison to Previous Run
Previous run (30 docs, single corpus): **F1=0.147**
Current run (7,570 docs, all corpora): **F1=0.365**
**Improvement: 2.5x** - More diverse training data helps!

---

## 3. DistilBERT Model (Baseline Config)

### Configuration
```yaml
Model: distilbert/distilbert-base-cased
Architecture:
  - Parameters: 65,194,757
  - Max sequence length: 512 tokens
  - Num labels: 5 (B-DIRECT, I-DIRECT, B-INDIRECT, I-INDIRECT, O)

Training:
  batch_size: 32
  epochs: 3
  learning_rate: 2.0e-5
  warmup_steps: 100
  gradient_accumulation_steps: 1
  bf16: true (mixed precision)

Optimizer: AdamW (standard for transformers)
Scheduler: Linear decay with warmup
```

### Training Data
```
Dataset: wright-american-fiction (single corpus)
Total Documents: 2,876 (all used)
Corpus Directory: ../corpora/wright-american-fiction

Data Split:
  Train: 2,013 documents (~70%)
  Val:   431 documents (~15%)
  Test:  432 documents (~15%)

Total Paragraphs: ~5,268 (after filtering)
Training samples: ~3,687 paragraphs
```

### Training Process
```
Device: CUDA (AMD GPU via ROCm 7.1)
Mixed Precision: BF16
Training time: ~45 minutes (3 epochs)

Epoch 1: Loss=0.0275
Epoch 2: [In progress]
Epoch 3: [Pending]
```

### Expected Results
Based on similar NER tasks:
- **Expected F1**: 0.5-0.7 (should outperform CRF)
- **Expected Precision**: 0.7-0.9
- **Expected Recall**: 0.5-0.7

**Advantages over CRF:**
- Pre-trained language understanding
- Contextual embeddings (knows "said" is a speech verb)
- Larger capacity for complex patterns
- Transfer learning from massive text corpus

---

## Performance Comparison

### F1 Score (Primary Metric)
| Model | F1 | 95% CI | Status |
|-------|-----|--------|--------|
| Quote Baseline | 0.0003 | N/A | âœ… Failed |
| CRF Fast | **0.365** | **[0.198, 0.519]** | âœ… Best so far |
| DistilBERT | TBD | TBD | ðŸ”„ Training |

### Precision-Recall Trade-off
```
Quote Baseline:  Very low precision (0.0002), low recall (0.0075)
                 â†’ Over-predicts massively

CRF Fast:        High precision (0.686), moderate recall (0.336)
                 â†’ Conservative, accurate when it predicts
                 â†’ Misses many speech tokens (low recall)

DistilBERT:      TBD
                 â†’ Expected to balance precision and recall better
```

### Training Time Comparison
| Model | Training Time | Hardware |
|-------|--------------|----------|
| Quote Baseline | 0s (no training) | N/A |
| CRF Fast | ~20 minutes | CPU (2 cores) |
| DistilBERT | ~45 minutes | GPU (BF16) |

### Model Size Comparison
| Model | Disk Size | Parameters |
|-------|-----------|------------|
| CRF Fast | ~1-2 MB | N/A (statistical model) |
| DistilBERT | ~250 MB | 65M |

---

## Statistical Analysis

### CRF Model Confidence Intervals

The 95% confidence intervals are relatively wide due to:

1. **High variance across folds**:
   - Fold 3: F1=0.636 (excellent)
   - Fold 5: F1=0.051 (poor)
   - Range: 12.5x difference!

2. **Data heterogeneity**:
   - Different corpora have different characteristics
   - Some corpora may have more/less speech
   - Different writing styles across time periods

3. **Class imbalance**:
   - Speech tokens are only ~0.7% of all tokens
   - Bootstrap sampling amplifies variance

### Is F1=0.365 Statistically Significant?

Compared to baseline (F1â‰ˆ0.0003):
- **Yes!** Even the lower CI bound (0.198) is 660x higher than baseline
- The model has clearly learned something meaningful

However, the wide CI suggests:
- Model performance varies by corpus
- Need to understand which corpora are harder
- May need corpus-specific tuning

---

## Recommendations

### For Production Use

1. **Use CRF Fast** for now:
   - Best performance available (F1=0.365)
   - Fast inference (no GPU needed)
   - Small model size (~1-2 MB)
   - interpretable features

2. **Wait for DistilBERT**:
   - Expected to outperform CRF
   - Will provide better balance of precision/recall
   - Requires GPU for inference

3. **Improve Recall**:
   - Current recall (0.336) is low
   - Consider weighted loss or oversampling
   - Add more speech verb features
   - Tune classification threshold

### For Future Development

1. **Corpus-Specific Analysis**:
   - Which corpora achieve best/worst performance?
   - Should we train separate models per corpus?

2. **Error Analysis**:
   - What types of speech does CRF miss?
   - What causes false positives?
   - Look at examples from Fold 3 (best) vs Fold 5 (worst)

3. **Feature Engineering**:
   - Add more linguistic features
   - Include syntactic dependencies
   - Add speaker attribution features

4. **Threshold Tuning**:
   - Current threshold favors precision
   - Lower threshold would increase recall
   - Find optimal operating point for use case

---

## Appendix: Raw Results

### CRF Model (`results/crf_fast/crf_results.json`)
```json
{
  "f1": {
    "estimate": 0.365,
    "ci_lower": 0.198,
    "ci_upper": 0.519,
    "se": 0.096,
    "fold_scores": [0.296, 0.424, 0.636, 0.416, 0.051]
  },
  "precision": {
    "estimate": 0.686,
    "ci_lower": 0.302,
    "ci_upper": 0.991,
    "se": 0.197,
    "fold_scores": [0.430, 0.985, 0.986, 1.000, 0.027]
  },
  "recall": {
    "estimate": 0.336,
    "ci_lower": 0.251,
    "ci_upper": 0.422,
    "se": 0.051,
    "fold_scores": [0.225, 0.270, 0.470, 0.263, 0.449]
  }
}
```

### Configuration Files Used
- `config/crf_fast.yaml` - CRF model configuration
- `config/distilbert_baseline.yaml` - DistilBERT configuration
- `../datasets/splits.json` - Data splits for all 7 corpora

### Data Splits
```
Total: 10,819 documents across 7 corpora
Train: 7,570 documents (22,749 paragraphs)
Val:   1,620 documents
Test:  1,629 documents (444,790 paragraphs)
```
