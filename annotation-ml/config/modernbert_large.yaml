# ModernBERT Model Configuration - Large Context
# Long-context transformer for speech detection

model:
  # Model identifier
  model_name: answerdotai/ModernBERT-base

  # Model architecture (long context!)
  max_length: 4096
  num_labels: 5

  # Training hyperparameters
  batch_size: 8  # Smaller due to long sequences
  epochs: 3
  learning_rate: 2.0e-5
  warmup_steps: 100
  gradient_accumulation_steps: 2  # Accumulate for effective batch size

  # Precision
  bf16: true

data:
  # Path to TEI XML corpus (transformers use flat structure, not splits)
  corpus_dir: ../datasets/wright-american-fiction

  # Maximum documents to load (null = all)
  max_docs: null

  # Train/val/test split
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15

training:
  # Logging
  logging_steps: 100

output:
  # Output directories
  results_dir: results/modernbert_large
  checkpoint_dir: checkpoints/modernbert
  save_predictions: true
